\chapter{Wstęp}


Od kilku lat zauważalny jest trend ku autonomizacji jazdy samochodem.  
Ma to na celu wyeliminowanie czynnika, który jest najczęstszą przyczyną wypadków -- człowieka. Dodatkowym aspektem, który wpływa na rozwój przemysłu motoryzacyjnego, są rosnące koszty transportu. 
Zastosowanie autonomicznych ciężarówek poruszających się w konwojach znacząco zmniejszyłoby koszty logistyki dla firm spedycyjnych. 
Systemy, które wspomagają lub wręcz wyręczają kierowcę z obowiązku kierowania uwagi na samochód i jego otoczenie noszą miano ADAS (\textit{Advanced Driver Assistance Systems} -- zaawansowane systemy wspomagania kierowcy). 
W systemach wspomagających kierowcę stosuje się różne czujniki, które pozwalają monitorować przestrzeń wokół pojazdu. 
Korzysta się z kamer pracujących w paśmie widzialnym i podczerwieni, radarów oraz lidarów. 
Radar oraz lidar to czujniki aktywne, które działają na podobnej zasadzie, lecz korzystają z fal elektromagnetycznych o różnej długości.
Radar pracuje w paśmie mikrofal, natomiast lidar korzysta z~pasma podczerwonego.
Przykładowe systemy korzystające z przetwarzania obrazów to detekcja pasa ruchu lub wykrywanie zmęczenia kierowcy. 
Bardzo popularne jest również stosowanie fuzji danych. 
Polega ona na jednoczesnym przetwarzaniu danych z dwóch lub więcej czujników, co zapewnia lepszą skuteczność algorytmu. 
Przykładem fuzji danych jest przetwarzanie danych z kamery i lidaru w celu detekcji pieszych.
Natomiast jednoczesna analiza danych pozyskanych z kamery i radaru używana jest w systemach aktywnego tempomatu lub detekcji pojazdów poprzedzających.


\section{Cele i założenia}

Celem niniejszej pracy było stworzenie aplikacji umożliwiającej testowanie algorytmów wizyjnych za pomocą symulatora ciężarówki.
Rozwiązanie to mogłaby być wykorzystane w ramach ćwiczeń laboratoryjnych poruszających problematyką systemów wizyjnych stosowanych w pojazdach autonomicznych prowadzonych w ramach kierunku Automatyka i Robotyka na Akademii Górniczo-Hutniczej.

\section{Istniejące systemy}
Aplikacje służące do testowania algorytmów wizyjnych są używane w przemyśle. 
Obecnie produkowane samochody są wyposażone w systemy, które bazując na widoku z kamery pomagają prowadzić samochód. 
Testowanie tych algorytmów jest niezwykle istotne, ponieważ niewykrycie błędów może prowadzić do katastrofalnych skutków, w tym potencjalnej śmierci człowieka. 
Przykładowo niewystarczające testowanie algorytmów w komercyjnym samochodzie Tesla 3 doprowadziło do śmiertelnego wypadku, ponieważ tzw. ,,autopilot'' nie wykrył poprawnie naczepy ciężarówki.

Popularne jest podejście do testowania SIL (ang. System in the loop - system w pętli), gdzie algorytm nie jest testowany w rzeczywistym pojeździe, lecz w oparciu o wcześniej nagrany przejazd samochodem testowym. Ocenie podlegają decyzje podejmowane przez algorytm w oparciu o nagranie video. Jest to rozwiązanie bezpieczniejsze oraz zdecydowanie tańsze. Wolumeny nagrań w bazach firm zajmujących się systemami ADAS sięgają milionów kilometrów jazd testowych.

\section{Zawartość pracy}

Rozdział drugi rozpoczyna się opisem poziomów autonomiczności samochodów. 
Ukazuje czym powinien charakteryzować się samochód, który znajduje się na określonym poziomie.
Następnie opisane są wybrane systemy wizyjne używane w branży automotive takie jak: detekcja pasa ruchu, detekcja świateł drogowych, wykrywanie znaków oraz detekcja samochodu poprzedzającego.
Opis jest opracowany na podstawie przeglądu publikacji naukowych.
Rozdział kończy krótki opis podstawowych operacji cyfrowego przetwarzania obrazów, których wyjaśnienie uznano za istotne z uwagi na ich częste stosowanie w systemach wizyjnych pojazdów autonomicznych.

Następny rozdział zawiera opis zrealizowanego systemu. 
Na początku dokonana jest ewaluacja dostępnych symulatorów jazdy wraz z uzasadnieniem wyboru Euro Truck Simulator 2. 
Kolejno opisana została architektura zaimplementowanej aplikacji. 
W~następnej części omówiono przykładowe algorytmy zaimplementowane w celu przetestowania i weryfikacji aplikacji stworzonej w ramach pracy. 
Końcowa część to sprawdzenie wydajności aplikacji w różnych sytuacjach.

Ostatni rozdział zawiera krótkie podsumowanie oraz ocenę przydatności aplikacji do prowadzenia zajęć dydaktycznych.

\chapter{Systemy wizyjne w pojazdach autonomicznych}

Współczesne pojazdy autonomiczne wykorzystują w~szerokim zakresie systemy wizyjne do analizy otoczenia. 
Jednym z pierwszych zastosowań algorytmów cyfrowego przetwarzania obrazów była detekcja pasa ruchu, która nie służyła do sterowania samochodem, lecz miała za zadanie wspomagać kierowcę w sytuacjach zmęczenia lub utraty koncentracji. 
Wraz z rozwojem technologii pojazdów autonomicznych, zaawansowanie systemów wizyjnych rosło od wspomnianej kontroli pasa ruchu, poprzez rozpoznawanie obiektów, które można spotkać na drogach: pieszych, pojazdów i rowerzystów, a także elementów infrastruktury: znaków i świateł drogowych.
W kolejnych podrozdziałach zostaną opisane czujniki stosowane w przemyśle motoryzacyjnym. Następnie zawarty jest krótki opis poziomów autonomiczności pojazdów. Rozdział kończy opis działania algorytmów wizyjnych w samochodach.

%TODO Dalej opis czujników nie jest w osobnym \section i tekst powyżej się nie zgadza.

\section{Poziomy autonomiczności}

Całość systemów, które wspomagają kierowcę w trakcie jazdy nazwano ADAS (\textit{Advanced Driver Assistance Systems}). 
Aby usystematyzować i podzielić poziom wpływu systemów na sterowanie pojazdem wprowadzono poziomy autonomiczności jazdy.

%TODO to do osobnego section
\begin{figure}
  \centering
  \includegraphics[width=13cm]{img/systemy_autonomiczne_ogolnie.png}
  \caption{Ogólny schemat kamer i radarów we współczesnych pojazdach autonomicznych\cite{S1}}
  \label{fig:kamery_i_radary}
  % https://aindustryreports.com/2019/05/23/advanced-driver-assistance-system-sensor-market-technological-innovations-in-north-america-to-boost-regional-market-attractiveness-through-2026/
\end{figure}

Na rysunku \ref{fig:kamery_i_radary} daje się zauważyć znaczną liczbę kamer i radarów wspomagająca kierowcę. 
Z przodu i tyłu pojazdu umieszczone są czujniki ultradźwiękowe (oznaczone na czerwono), których zadaniem jest wspomaganie procesu parkowania samochodu poprzez wykrywanie obiektów w bliskim otoczeniu pojazdu.
Pozostając przy parkowaniu, kamera umieszczona z tyłu pojazdu również wspomaga parkowanie pokazując kierowcy obraz przestrzeni za samochodem. 
Co raz popularniejsze są systemy czterech kamer -- po jednej na każdą krawędź samochodu. 
Na podstawie obrazu pochodzącego z nich generowany jest obraz samochodu z lotu ptaka.
Kamery są również używane w systemie asystenta pasa ruchu oraz detekcji i rozpoznawania znaków drogowych.
Radary bliskiego zasięgu (jasnozielony) i lidary (ciemnoszary) znalazły zastosowanie w komponentach odpowiedzialnych za sytuacje awaryjne takie jak system unikania kolizji czy detekcja martwego pola.
Radar dalekiego zasięgu jest używany w tempomacie adaptacyjnym.
Powszechną techniką jest stosowanie redundancji w krytycznych systemach, co jest regulowane poprzez normy bezpieczeństwa ISO 26262.

%TODO Tu powinien się zacząć rodział o autonomiczności... (section)

\subsection{Poziom zerowy -- brak autonomiczności}

Klasyfikacja została opracowana na podstawie \cite{S3}. %TODO poza subsection

Obecnie większość samochodów na drodze zaliczanych jest do tego poziomu. 
Samochód jest w pełni kontrolowany przez człowieka, chociaż mogą pojawiać się proste systemy, które mogą pomóc kierowcy np. system awaryjnego hamowania. 
Dopóki bezpośrednio nie wpływa on na tor jazdy, nie jest to system autonomiczny. 
Innym przykładem jest ABS (\textit{Anti-lock brake system} -- system zapobiegający blokowaniu kół podczas hamowania), który również nie jest systemem, który zapewniałby autonomiczność pojazdowi.
Służy on poprawie bezpieczeństwa hamowania i jego działanie opiera się tylko na odczycie danych z czujników niezależnie od aktualnej sytuacji na drodze i wokół pojazdu. 
System zadziała poprawnie na drodze asfaltowej w przypadku awaryjnego hamowania i pozwoli bezpiecznie ominąć przeszkodę (tj. zapobiega blokadzie kół i~tym samym poślizgowi pojazdu). 
Praktyka pokazuje, że w przypadku, gdy zajdzie potrzeba nagłego zatrzymania samochodu na oblodzonej drodze lepiej jest, aby system ABS nie zadziałał, ponieważ znacząco wydłuża on drogę hamowania. 
Jako, że samochody poziomu zerowego nie interpretują stanu otoczenia, wspomniany system będzie działał zawsze, niezależnie od tego czy przyniesie to pozytywny efekt. 

%TODO ESP - może już nie, bo tego to całkiem dużo by było.

\subsection{Poziom pierwszy -- asystenci jazdy}
%to jest kalka z angielskiego, gdzie mamy "driver assists". W polskim pojawiają się asystenci jazdy (jako systemy wspomagające kierowcę), którzy jak rozumiem pełnią asystę. Zmieniam na asystenci jazdy, oczywiście do ew. zmiany
%TODO no może asystent brzmi ciut lepiej, ale też szału nie ma :). Niech będzie.

Jest to najniższy poziom autonomiczności. 
Auto posiada pojedynczy system wspomagania kierowcy, taki jak zmienianie kierunku jazdy samochodu lub przyspieszanie (tempomat). 
Przykładem systemu, który jest montowany w samochodach autonomicznych poziomu pierwszego jest adaptacyjny tempomat, czyli system, który zachowuje bezpieczny dystans od poprzedzającego pojazdu. 
Kierowca kontroluje pozostałe aspekty jazdy samochodem takie jak kierowanie i hamowanie. 


\subsection{Poziom drugi -- częściowa automatyzacja jazdy samochodem}
Oznacza zaawansowanych asystentów jazdy. 
Samochód może sam kontrolować zarówno kierunek jazdy i przyspieszanie oraz hamowanie. 
Jest w stanie jechać samodzielnie, lecz wymaga ciągłej obecności kierowcy za kierownicą, który może w~każdej chwili przejąć kontrolę nad samochodem. 
Obecnie stosowane systemy jazdy autonomicznej takie jak np. Tesla Autopilot montowane w samochodach marki Tesla (rys. \ref{fig:teslas}) kwalifikują się jako poziom drugi.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{img/tesla.jpg}
  \caption{Tesla Model S -- pierwszy samochód z drugim poziomem zaawansowania systemów wspomagania kierowcy (\textit{źródło: Wikipedia})}
  \label{fig:teslas}
  % https://pl.wikipedia.org/wiki/Tesla_Model_S#/media/Plik:2014_Tesla_Motors_Model_S_(rear_view)_Netherlands.jpg
\end{figure}

\subsection{Poziom trzeci}

Granica pomiędzy poziomem trzecim, a drugi jest dość nieostra. 
Samochód posiada możliwość percepcji otaczającego go środowiska i~na podstawie zgromadzonych informacji samodzielnie podejmować decyzje. 
System jest jednak wciąż zależny od człowieka, który musi pozostać czujny i być w stanie zareagować, gdy system nie będzie w stanie podjąć decyzji. 
W~2019r. Audi wprowadziło model A8, który zapowiadano jako pierwszy samochód poziomu trzeciego. 
System \textit{Traffic Jam Pilot} bazując na danych z lidaru oraz kamer zapewniał autonomiczną jazdę w korkach. 
Problemem okazał się jednak brak odpowiednich regulacji prawnych. 
W Stanach Zjednoczonych stosowanie tego systemu było zabronione, więc samochód w okrojonej wersji sprzedawano jako pojazd autonomiczny drugiego poziomu. 
Na rynku europejskim samochód pojawił się z zaimplementowaną funkcjonalnością asystenta jazdy w korku.

\subsection{Poziom czwarty}

Poziom czwarty zapewnia poprawną reakcję samochodu również w nagłych sytuacjach takich jak np. wypadek lub pęknięcie opony. 
Pojazdy nie wymagają ingerencji człowieka w znakomitej większości sytuacji, jednak kierowca zawsze powinien być w stanie przejąć kontrolę. 
Podobnie jak w poziomie trzecim problemem okazały się ograniczenia prawne. 
Samochody te mogą z reguły poruszać się na ograniczonym obszarze. 
Obecnie w fazie testów są samochody takie jak Waymo lub NAVYA.

\subsection{Poziom piąty -- pełna autonomia}

Samochody nie wymagają ingerencji ze strony człowieka. 
Najprawdopodobniej nie będą wyposażone w kierownicę ani pedały. 
Będą w stanie jechać gdziekolwiek, system sterowania będzie działał na poziomie doświadczonego kierowcy. 
Samochody te są obecnie w stanie wczesnych testów, jednak można się spodziewać, że w ciągu najbliższych kilkunastu lat pierwsze w pełni autonomiczne samochody będą pojawiać się na drogach.


\subsection{Autonomiczne samochody ciężarowe}

Warto zaznaczyć, że ważnym polem do zastosowania systemów autonomicznej jazdy jest transport ciężarowy. 
Systemy wspomagające kierowcę pojawiają się w samochodach ciężarowych analogicznie do samochodów osobowych. 
W najbliższych latach planuje się rozwijanie koncepcji autonomicznych samochodów ciężarowych poprzez tworzenie konwojów, w których kierowca znajduje się tylko w pierwszym samochodzie (źródło: \cite{W4}).
Kolejnym krokiem będzie autonomiczna jazda na dystansie trasy ze wsparciem kierowcy podczas załadunku i rozładunku. 
Ostatnim krokiem, podobnie jak w przypadku samochodów osobowych, będzie w pełni autonomiczna jazda bez udziału człowieka.

Systemy wizyjne stosowane w ciężarówkach nie odbiegają sposobem pracy od tych używanych w samochodach osobowych.



\section{Algorytmy wizyjne w pojazdach autonomicznych}

W~kolejnych podrozdziałach zostaną opisane algorytmy wizyjne, których zadaniem jest analiza otoczenia samochodu. 
W ostatnich latach dynamicznie rozwijającą się dziedziną w systemach wizyjnych i motoryzacji są sieci neuronowe, które są obecnie najczęściej wybierane do implementacji w samochodach. 
W pracy opisane zostaną wybrane algorytmy bazujące na klasycznych metodach przetwarzania obrazach cyfrowych oraz oparte na uczeniu głębokim. 

\subsection{Wykrywanie pasa ruchu}

\label{sec:lane_detection}

Wykrywanie pasa ruchu było jednym z pierwszych algorytmów implementowanych w samochodach komercyjnych. 
W trakcie jazdy kluczowym elementem jest utrzymanie samochodu w pasie jezdni niezależnie od stanu nawierzchni, pogody, czy prędkości (rys. \ref{fig:inputimg1}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/input.png}
  \caption{Przykładowy obraz wejściowy algorytmu detekcji pasa ruchu}
  \label{fig:inputimg1}
  \end{figure}

W~najczęstszym przypadku wejściem do algorytmu jest obraz z kamery umieszczonej pod pewnym kątem do nawierzchni zamontowanej w okolicy przedniego zderzaka, atrapy chłodnicy lub za lusterkiem wstecznym.
W niniejszej pracy, większość przetwarzanych obrazów pochodzi z symulatora Euro Truck Simulator 2, (który dokładniej jest opisany w rozdziale \ref{sec:ets2}), gry komputerowej, która posiada zaawansowaną grafikę, która przypomina otaczającą rzeczywistość, a także udostępnia możliwości programistyczne, które ułatwią stworzenie aplikacji do testowania algorytmów wizyjnych, co jest założeniem niniejszej pracy.

W niniejszym podrozdziale zostanie opisany algorytm z artykułu \cite{T3}. 
Autor korzysta w nim z podstawowych operacji przetwarzania obrazów. 

\begin{figure}
  \centering
  \includegraphics[width=10cm]{img/roi.png}
  \caption{Linie obrazujące granice obszaru poddawanego analizie\cite{T3}}
  \label{fig:roi} 
  % artykuł
\end{figure}

Pierwszym krokiem jest wyodrębnienie z obrazu ROI \footnote{ROI (ang.) -- \textit{Region of Interest} -- obszar obrazu poddawany analizie i dalszemu przetwarzaniu} (rys. \ref{fig:roi}). 
Ma to na celu ograniczenie ilości danych poddawanych analizie, a co bardziej istotne odrzuca te fragmenty obrazu, na których na pewno nie będzie jezdni (powyżej czerwonej linii), a także te gdzie obraz linii mógłby być zakłócony. 
Warto zauważyć, że pomiędzy liniami -- czerwoną i niebieską znajduje się prawie dwa razy więcej długości drogi niż w obszarze pod niebieską linią. Obszar pod niebieską linią jest odrzucany, ponieważ na dużym obszarze obrazu znajduje się niewielki fragment jezdni, co może generować problemy z poprawną detekcją w przypadku linii przerywanej.
%TODO OK, choć jak dla mnie argument dyskusyjny

Kolejnym etapem jest ekstrakcja linii rozdzielających pasy i~pobocze. 
W większości krajów mają one kolor biały, rzadziej żółty (czasem kolor żółty mają oznaczenia stosowane podczas remontów). 
Mając na wejściu obraz RGB\footnote{RGB -- obraz o~trzech składowych barwnych: czerwonej (R), zielonej (G) i~niebieskiej (B)} można zauważyć, że składowe czerwona i zielona mają większe wartości tam gdzie na obrazie są linie w porównaniu do nieoznakowanej nawierzchni jezdni. 

W~omawiany artykule zaproponowano następującą metodę segmentacji linii:
\begin{equation}
\label{eq:IMij}
IM(i,j)=\left\{\begin{matrix}
255, & \begin{matrix}
R(i,j)\geq (0.2R_{min}+0.8R_{max})\\ 
G(i,j)\geq (0.2G_{min}+0.8G_{max})
\end{matrix}\\ 
0, & wpp.
\end{matrix}\right.
\end{equation}
%TODO wpp - no raczej wiadomo o co chodzi.

\begin{equation}
\label{eq:Gij}
G(i,j)=\left\{\begin{matrix}
255, & \begin{matrix}
R(i,j)\geq G(i,j) \geq B(i,j)\\ 
IM(i,j)>0
\end{matrix}\\ 
0, & wpp.
\end{matrix}\right.
\end{equation}

\begin{equation}
\label{eq:Gray}
Gray(i,j)=R(i,j)+G(i,j)-2B(i,j)+0.3*8|R(i,j)+G(i,j)|
\end{equation}

\begin{equation}
\label{eq:GMij}
GM(i,j)=\left\{\begin{matrix}
128, &  Gray(i,j)\geq 0.8*Gray_{m} \\ 
255, & \begin{matrix}
2*Gray_{avg} \leq Gray(i, j) \leq Gray_{m}\\albo\\ 
G(i,j)=255
\end{matrix} \\ 
0,   & wpp.
\end{matrix}\right.
\end{equation}

W~równaniu \eqref{eq:IMij} \ ${IM(i,j)}$ oznacza tymczasową macierz cech koloru. ${R(i,j)}$ oznacza wartość składowej czerwonej, a~${G(i,j)}$ składową zieloną. 
${R_{max}}$, ${R_{min}}$, ${G_{max}}$, ${G_{min}}$ reprezentują maksymalną i ~inimalną wartość składowej czerwonej, a także maksymalną i~minimalną wartość składowej zielonej. 
W~równaniu \eqref{eq:Gray} \ $ Gray(i,j)$ oznacza obraz wejściowy w skali szarości, na wartości którego ma wpływ każda ze składowych barwnych. 
W~ostatnim równaniu \eqref{eq:GMij} \ $GM(i,j)$ oznacza obraz wynikowy, na którym zaznaczone jest wysegmentowane poziome oznaczenie jezdni. 
$Gray_{avg}$ to średnia wartość obrazu w skali szarości w danym wierszu, natomiast $Gray_{m}(i,j)$ oznacza wartość maksymalną dla danego wiersza.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/segmentacja.png}
  \caption{Obraz $GM$ -- wysegmentowane linie\cite{T3}}
  \label{fig:segmented}
\end{figure}


Kolejnym etapem opisywanego algorytmu jest wykrywanie krawędzi zrealizowane za pomocą filtru Canny'ego. 
Jest to filtr nieliniowy z histerezą, który dobrze sprawdza się do wykrywania krawędzi na obrazach niejednorodnych i~rozmytych. 
Następnie podejmowana jest ekstrakcja cech linii. 
Na potrzeby wyjaśnienia działania podanego fragmentu algorytmu przyjęto:
\begin{itemize}
\item $IME$ -- obraz z równania \eqref{eq:GMij} po filtracji filtrem Canny'ego
\item $IMC$ -- obraz $GM$ z równania \eqref{eq:GMij}
\end{itemize}

Dla każdego piksela w~obrazie $IME$, który został oznaczony jako krawędź, zostaje sprawdzona następująca zależność:
\begin{equation}
	\begin{matrix}
	IMC(i,j+1)+IMC(i,j)+IMC(i,j-1)+IMC(i-1,j+1)\\
	+IMC(i-1,j)+IMC(i-1,j-1)+IMC(i+1,j+1)+IMC(i+1,j-1)>0
	\end{matrix}
\end{equation}
Jeśli jest ona spełniona, wybrany punkt na obrazie $IME$ zostaje uznany jako krawędź pasa jezdni.
W praktyce ma to za zadanie zwiększyć skuteczność algorytmu. 
Linie oddzielające pasy ruchu z reguły mają na obrazie szerokość większą niż jeden piksel (a taką szerokość mają póki co na obrazie uzyskanym po detekcji krawędzi), więc jeśli choć jeden piksel z ośmiopikselowego otoczenia jest uznawany za krawędź, to rozważany piksel również jest traktowany jako element krawędzi.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/canny.png}
  \caption{Obraz po zastosowaniu filtra Canny'ego oraz ekstrakcji cech linii\cite{T3}}
  \label{fig:canny}
\end{figure}

Do ostatecznego wykrycia linii prostych należy użyć zmodyfikowanej transformaty Hougha (\textit{constraint Hough transform}). 
Główna różnica pomiędzy wykorzystywaną, a klasyczną transformatą Hougha polega na tym, że wartości $\rho$ i $\theta$ są skwantowane i pogrupowane. 
Dzięki takiej modyfikacji nieidealnie proste linie lub takie, które leżą blisko siebie nie będą wykryte wielokrotnie. 

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/prehough.png}
  \caption{Wynik algorytmu detekcji linii\cite{T3}}
  \label{fig:result}
\end{figure}

Czas przetwarzania jednego obrazu RGB o wymiarach 640x480 wynosi 64.5ms.

\subsubsection{Alternatywne podejście}

Rozwiązanie, które w odróżnieniu do przestawionego wyżej nie korzysta z transformaty Hougha, przedstawił Yue Dong w artykule \cite{T6}.
Linie rozdzielające pasy ruchu można rozpoznać po wysokim kontraście między nimi (\ref{fig:lane_detection2_input}), a nawierzchnią jezdni. 
Aby określić lokalizację pojazdu, należy dokładnie wyznaczyć pozycję linii. 
Podobnie jak w rozwiązaniu opisywanym w podrozdziale \ref{sec:lane_detection} pierwszym krokiem jest detekcja krawędzi z użyciem filtru Canny'ego (\ref{fig:lane_detection2_canny}). 
W dostępnych bibliotekach wartości progów mogą być dobierane automatycznie, lecz z reguły skutkuje to wykryciem dużej liczby krawędzi nieistotnych z punktu widzenia algorytmu np. należących do samochodów poprzedzających.

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[width=6cm]{img/lane_detection2_input.png}
		\subcaption{\label{fig:lane_detection2_input}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[width=6cm]{img/lane_detection2_canny.png}
		\subcaption{\label{fig:lane_detection2_canny}}
	\end{subfigure}
	
	\caption{\label{fig:details}Obraz wejściowy algorytmu \protect\subref{fig:lane_detection2_input} i obraz po detekcji krawędzi z użyciem filtru Canny'ego \protect\subref{fig:lane_detection2_canny}.\cite{T6}}
\end{figure}

Orientacja kamery względem drogi implikuje fakt, że linie będą na obrazie skośne. 
Następnym krokiem po detekcji krawędzi jest więc usunięcie pikseli, które stykają się z innymi pikselami w osi pionowej lub poziomej (rys. \ref{fig:lane_detection2_noise1}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection2_noise1.png}
  \caption{Rezultat redukcji linii poziomych i pionowych\cite{T6}}
  \label{fig:lane_detection2_noise1}
\end{figure}

Kolejnym etapem jest odrzucenie pojedynczych punktów, które nie mają połączenia z innymi (rys. \ref{fig:lane_detection2_noise2}).
% Autor artykułu niestety nie podaje na jakiej zasadzie są te punkty odrzucane, a takie proste odrzucenie pojedynczych punktów to chyba za proste by było..
\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection2_noise2.png}
  \caption{Wynik odrzucenia punktów pojedynczych, które nie mają połączenia z innymi\cite{T6}}
  \label{fig:lane_detection2_noise2}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection2_search_line.png}
  \caption{Linia skanu wraz z zaznaczonym obszarem poszukiwania linii (Search area)\cite{T6}}
  \label{fig:lane_detection2_search_area}
\end{figure}

W następnym kroku wyznaczany jest obszar oraz linia skanu (rys. \ref{fig:lane_detection2_search_area}). 
Na wspomnianej linii badana jest jasność pikseli (rys. \ref{fig:lane_detection2_intensity}). Zauważalne są dwa duże skoki wartości, które odpowiadają linii na jezdni.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection2_intensity.png}
  \caption{Widoczne dwa skoki wartości jasności oznaczające linie pasa ruchu\cite{T6}}
  \label{fig:lane_detection2_intensity}
\end{figure}

Taka sama operacja jest powtarzana dla ostatniego rzędu pikseli obrazu. 
Jeśli tam również istnieją dwa maksima, to na podstawie współrzędnych wyznaczana jest linia, która oznacza detekcję pasa ruchu (rys. \ref{fig:lane_detection2_result}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection2_result.png}
  \caption{Rezultat algorytmu wykrywającego linie oddzielające pasy ruchu\cite{T6}}
  \label{fig:lane_detection2_result}
\end{figure}

Metoda jest wrażliwa na linie przerywane lub inne znaki poziome pojawiające się na jezdni. Proponowanym rozwiązaniem jest wprowadzenie śledzenia linii lub pamiętanie poprzednich detekcji w przypadku niewykrycia pasa ruchu.
W porównaniu do pierwszego opisanego algorytmu, ten wykonuje się szybciej (przetwarzanie obrazu RGB o wymiarach 640x480 trwa 0.12s)

\subsubsection{Rozwiązanie z użyciem metod uczenia głębokiego}
%TODO tu brakuje \cite w txt - jest tylko pod rysunkami

Rozwiązaniami konkurencyjnymi w stosunku do opartych o algorytmy wizyjne są te bazujące na sztucznej inteligencji. 
Większość metod klasycznych jest wrażliwa na zmiany jasności, warunki pogodowe, zakłócenia, więc zawodzą, gdy środowisko zewnętrze jest zbyt zmienne. 
W ostatnich latach metody oparte na DCNN (ang. \textit{Deep Convolutional Neural Networks} - głębokouczone konwolucyjne sieci neuronowe) przewyższają te oparte na podejściu tradycyjnym w wielu aplikacjach. 
Przykładem użycia sieci neuronowych jest detekcja linii oddzielających pasy ruchu. 
Schemat działania systemu opartego o uczenie głębokie jest umieszczony na rys. \ref{fig:lane_detection3_cnn_general}.
\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection3_cnn_general.png}
  \caption{Schemat działania aplikacji opartej o DCNN\cite{T6}}
  \label{fig:lane_detection3_cnn_general}
\end{figure}

W pierwszym kroku sieć wyznacza pozycję linii na obrazie. 
Pozostałe dwa etapy opierają się na tradycyjnym podejściu. 
W typowym użyciu CNN próbkowanie jest używane między innymi w celu umożliwienia znajdowania dużych obiektów. %TODO to jest niejasne - chodzi o zmianę rozdzielczości ?
Wadą takiego rozwiązania jest zmniejszenie szczegółowości informacji otrzymanej z sieci.
Po segmentacji linii kolejnym etapem jest segmentacja semantyczna. 
Polega ona na określeniu, czym są obiekty wysegmentowane. 
Opisywane rozwiązanie określa trzy możliwości: linia lewa, linia prawa i nawierzchnia. 
Ostatnim etapem jest ewentualne grupowanie linii w przypadku elementów przerywanych.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/lane_detection3_cnn_results.png}
  \caption{Efekty działania aplikacji opartej na DCNN. Obraz wejściowy (a), segmentacja (b), segmentacja semantyczna (c), wykryte linie(d)\cite{T6}}
  \label{fig:lane_detection3_cnn_results}
\end{figure}

Średni czas przetwarzania jednego obrazu o wymiarach 480x360 wynosi 30ms. Jest to wynik porównywalny z algorytmami wizyjnymi.

%TODO Dlaczego zakomentowane ? Artykuł, na którym to jest oparte zawierał pewne luki i nie byłem w stanie dociec o co tak naprawdę autorom chodzi - TK:OK

%\subsection{Detekcja zakrętów}
%Podobnym zadaniem jest detekcja pasa ruchu w zakręcie. Kształt zakrzywionej linii może być opisany za pomocą paraboli z równania \ref{eq:ld_1}. Zauważono, że fragment drogi bliżej samochodu z reguły jest prosty, zakrzywienie widać powyżej pewnej odległości, a na obrazie powyżej pewnej wysokości. Równanie \ref{eq:ld2} pokazuje model pasa ruchu, gdzie $y_m$ to współrzędna punktu, w którym prosta przechodzi w krzywą.
%\begin{equation}
%x=a+by+cy^{2}
%\end{equation}
%
%\begin{equation}
%x=\begin{cases}
%a+by,\qquad\qquad y\leq y_{m}\\ c+dy+ey^{2},\qquad y > y_{m} \end{cases}
%\end{equation}
%
%gdzie ponadto:
%\begin{itemize}
%\item $a,b$ -- parametry linii
%\item $c,d,e$ -- parametry krzywej
%\end{itemize}
%
%Linie prosta i krzywa powinny spotykać się w jedym punkcie, którego współrzędna wysokościowa jest równa $y_m$. Po pewnych przekształceniach otrzymano równanie \ref{eq:ld_3} i \ref{eq:ld_4}.
%
%\begin{equation}
%a+by_{m}=c+dy_{m}+ey_{m}^{2}
%\end{equation}
%
%\begin{equation}
%b = d+2ey_m
%\end{equation}
%
%Ostatecznie uzyskano zależność \ref{eq:ld_5} pomiędzy parametrami krzywych.
%
%\begin{equation}
%\begin{cases} c=a+\frac{y_{m}}{2}(b-d) \\
%e=\frac{1}{2y_{m}}(b-d) \end{cases}
%\end{equation}

%TODO KONIEC CZYTANIA TK

\subsection{Detekcja znaków drogowych}

Innym w swojej naturze zadaniem stawianym przed systemami wizyjnymi w pojazdach autonomicznych jest detekcja i rozpoznawanie znaków drogowych. %TODO i rozpoznawanie OK
Systemy tego typu pojawiały się dość wcześnie w~samochodach pełniąc jedynie funkcję ostrzegawczo-informacyjną.
Coraz bardziej dynamicznie rozwijające się systemy wspomagające kierowcę wymagają od podsystemu odpowiedzialnego za detekcję znaków drogowych dużej skuteczności, ponieważ na ich efektach pracy opierają decyzje o sterowaniu pojazdem. %TODO podsystem... powt. detekcja OK
W przypadku tego zagadnienia istotnym problemem jest brak ujednoliconego zestawu znaków dla całego świata. %TODO w przypadku tego zagadnienia OK
Obecnie każdy kraj posiada swój zestaw znaków, które w ogólności są podobne, jednak różnice w szczegółach komplikują projektowanie opisywanych systemów. %TODO może komplikują projetkowanie OK
Proponowanym rozwiązaniem jest zbudowanie odpowiednio dużej bazy wzorców znaków i~stosowanie określonego zestawu w zależności od konkretnej lokalizacji. 

Rozwiązanie proponowane w~pracy \cite{T2} składa się z dwóch etapów. 
Pierwszy to detekcja znaku, a drugi to jego klasyfikacja. 
Do wykrycia znaku drogowego na obrazie pochodzącym z kamery umieszczonej w samochodzie użyto metody \textit{Radial Symmetry Transform} do detekcji znaków ograniczenia prędkości. 
Obecnie stosowane metody detekcji znaków drogowych bazują na segmentacji koloru lub ekstachowania cech kształtu. %TODO zdanie niejasne OK

\subsubsection{Symmetry Transform}

Klasyczne detektory kształtu wymagają często zamkniętych konturów. 
Techniki odporne, takie jak transformata Hougha dla okręgów wymaga dużych nakładów obliczeniowych dla dużych obrazów. 
Metoda \textit{\textit{fast radial symmetry detector} -- (ang. szybki detektor symetrii radialnej) -- FRSD} może być używana w~czasie rzeczywistym. %TODO a to nie powinno być detektor symetrii radialnej ? OK
Znakomita większość znaków z ograniczeniem prędkości to koło z czerwonym brzegiem i wartością ograniczenia w środku na białym tle. 
Opisywana metoda detekcji jest kompatybilna ze wszystkimi głównymi metodami klasyfikacji takimi jak np. SVM (ang. \textit{Support Vector Machine} -- maszyna wektorów wspierających). 
Zaletą użycia opisywanego algorytmu jest fakt, że wraz z informacją o wykrytym znaku podawana jest skala znaku, co znacząco ułatwia klasyfikację, ponieważ niepotrzebne staje się używanie szablonów o wielu rozmiarach dla wielu różnych rozdzielczości. %TODO nie wiem czy główną, czy dodatkową. Ponadto proszę spróbować mniej słowa "metoda". OK

Detektor radialnej symetrii jest wariantem transformaty Hougha dla okręgów. 
Jest on wykonywany w porządku $kp$, gdzie $k$ oznacza liczbę promieni, które są szukane, a~$p$ liczbę pikseli. Stanowi to różnicę w stosunku do klasycznej transformaty Hougha wykonywanej w porządku $kbp$, gdzie każdy piksel na obrazie krawędzi ,,głosuje'' dla każdego koła z dyskretnego zestawu promieni. $b$ oznacza dyskretyzację zestawu promieni okręgów, które mogą przechodzić przez aktualnie analizowany punkt.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/fsrd1.png}
  \caption{Algorytm głosowania - dla danego punktu krawędzi środki rodziny krawędzi leżą na lini prostopadłej do krawędzi\cite{T2}}
  \label{fig:frsd1}
\end{figure}

FRSD eliminuje czynnik $b$ poprzez pozyskanie informacji o kierunku gradientu z detektora krawędzi Sobela.
Zamiast sprawdzania każdego możliwego kierunku promienia, sprawdzany jest jedynie kierunek prostopadły do kierunku gradientu, co jest widoczne na rysunku \ref{fig:frsd1}. 
Powoduje to, że przestrzeń rozwiązań z~trójwymiarowej staje się dwuwymiarowa, co pozwala na używanie algorytmu w czasie rzeczywistym. 
Operacja detekcji symetrii radialnej może być prosto zrozumiana jako rozważenie wszystkich możliwych okręgów, których dany piksel może być częścią, jeżeli znany jest kierunek krawędzi, to okręgi są szukane tylko na linii prostopadłej do krawędzi (rys. \ref{fig:frsd1}). %TODO to może dać obok tego poprzedniego rysunku. detekcja symetrii radialnej OK



\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/fsrd2.png}
  \caption{Działanie FRSD dla okręgu, zauważalne przecięcie średnic skutkuje istnieniem maksimum lokalnego w środku okręgu.\cite{T2}} %TODO Zachowanie - działanie OK
  \label{fig:frsd2}
\end{figure}
Na rysunku \ref{fig:frsd2} widać działanie algorytmu dla okręgu. Badanie wartości gradientu tylko na kierunku prostopadłym do orientacji gradientu implikuje istnienie maksimum w środku okręgu.


Praktyczna realizacja jest wykonywana na obrazie cyfrowym, więc promień jest skwantowany na kilka przedziałów długości. %TODO niejasne OK
Istotne jest poprawne dobranie ograniczeń na długość promienia. 
Na rysunku \ref{fig:tsd} po prawej stronie jest widoczny znak ograniczenia prędkości, który powinien zostać wykryty. 
Daje się zauważyć, że można dobrać odpowiednie zakresy promienia dla znaków, które będą pojawiać się na obrazie z kamery. Można również wyodrębnić obszar, na którym znaki na pewno nie będą się pojawiać (jezdnia przed pojazdem, wyższa część nieba). %TODO to zdanie jest takie dziwne - pełna zgoda OK
\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/znaki1.png}
  \caption{Obraz wejściowy algorytmu detekcji znaków drogowych\cite{T2}}
  \label{fig:tsd}
\end{figure}

%\begin{figure}
%\centering
%\subfloat[obraz 1]{\label{fig:znaki2}
%\includegraphics[width=0.3\textwidth]{img/znaki2.png}}
%\quad
%\subfloat[obraz 2]{\label{fig:znaki3}
%\includegraphics[width=0.3\textwidth]{img/znaki3.png}}
%\caption{Wyniki detekcji, gdy zakres promieni jest zbyt mały \protect\subref{subfigure_a} i zbyt duży, \protect\subref{subfigure_b}.}
%\label{fig:tsd1}
%\end{figure}

\begin{figure}[h]
	\centering
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[width=6cm]{img/znaki2.png}
		\subcaption{\label{fig:znaki2}}
	\end{subfigure}
	\begin{subfigure}{0.35\textwidth}
		\centering
		\includegraphics[width=6cm]{img/znaki3.png}
		\subcaption{\label{fig:znaki3}}
	\end{subfigure}
	
	\caption{\label{fig:details}Wyniki detekcji, gdy zakres promieni jest zbyt mały \protect\subref{fig:znaki2} i zbyt duży \protect\subref{fig:znaki3}.\cite{T2}}
\end{figure}

Wszystkie opisywane operacje są wykonywane na obrazach dyskretnych, więc przestrzeń możliwych długości promieni jest skwantowana. W przypadku, gdy dobrany zakres długości promieni będzie zbyt mały lub zbyt duży, detektor ma gorszą sprawność. Dzieje się tak, ponieważ ewentualne maksima są rozmyte (rys. \ref{fig:znaki2} i rys. \ref{fig:znaki3})
%TODO Brak opisu tych rysunków. OK 
%TODO Brak opisu rozwiązania "konkurencyjnego". OK
%TODO Swoją drogą komentarz co dla innych znaków. - w konkurencyjnym rozwiązaniu - powyższy artykuł był opisany tylko dla okrągłych znaków zakazu.

\subsubsection{Rozwiązanie alternatywne}
Rozwiązanie zaproponowane przez I.M. Creusena w \cite{T7} pozwala na detekcję nie tylko znaków zakazu. Dodatkową wartością opisywanego rozwiązania jest korzystanie z dużego zbioru obrazów panoramicznych robionych w odstępach dostępnego w ramach projektu Google Maps. Każde zdjęcia ma do siebie przypisane współrzędne GPS, więc rezultaty detekcji można umieścić np. w bazie nawigacji satelitarnych.

\begin{figure}
  \centering
  \includegraphics[width=13cm]{img/sign_detection2_input.png}
  \caption{Panoramiczny obraz wejściowy algorytmu detekcji znaków drogowych\cite{T7}}
  \label{fig:sign_detection2_input}
\end{figure}

Podobnie jak w przypadku pierwszego opisywanego algorytmu dotyczącego znaków drogowych, ten również składa się z dwóch głównych etapów: detekcji i klasyfikacji.

Zmiana przestrzeni barw jest przydatna w przypadku algorytmów detekcji znaków drogowych, jednak prosta zmiana przestrzeni nie jest wystarczająca, by zbudować system odporny na zmienne warunki. Problem polega na tym, że metody oparte na segmentacji koloru są wrażliwe na jasność tła. Dzieje się tak z powodu zmienności kierunku gradientu w zależności od stosunku jasności znaku do tła. W przypadku ciemnego tła kierunek gradientu jasności jest odwrotny niż w przypadku jasnego tła, chociaż charakter znaku jest identyczny (\ref{fig:sign_detection2_example}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/sign_detection2_example.png}
  \caption{Przykład zmienności kierunku gradientu w przypadku ciemnego i jasnego tła\cite{T7}}
  \label{fig:sign_detection2_example}
\end{figure}

Pierwszym krokiem algorytmu jest wybór przestrzeni barw, do której zostanie przekonwertowany obraz. W opisywanym rozwiązaniu zdecydowano się na CIElab. Kolejnie ustalane są kolory odniesienia w danej przestrzeni dla zestawu znaków: czerwony, niebieski, żółty.
Określona jest różnica pomiędzy kolorem odniesienia, a kolorem piksela obrazu:
\begin{equation}
p_t=\left \| \bar{p} - \bar{p_r} \right \|
\end{equation}

gdzie
\begin{itemize}
\item{$\bar{p}$ - oznacza wektor koloru pojedynczego piksela obrazu analizowanego w przestrzeni CIElab}
\item{$\bar{p_r}$ - oznacza kolor odniesienia w tej samej przestrzeni}
\end{itemize}

Gdy różnica jest obliczona jest interpretowana jako współrzędne biegunowe (bez rozważania kąta), których początkiem współrzędnych jest kolor odniesienia. Rysunek \ref{fig:sign_detection2_result} pokazuje, że połączenie współrzędnych biegunowych i koloru odniesienia daje zadowalające rezultaty. Tło staje się jasne, a obwódki znaków ciemne. Dzieje się tak niezależnie od poziomu jasności tła.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/sign_detection2_result.png}
  \caption{Przykład zmienności kierunku gradientu w przypadku ciemnego i jasnego tła\cite{T7}}
  \label{fig:sign_detection2_result}
\end{figure}


\subsubsection{Rozwiązanie z użyciem deep learningu}
Chociaż konwolucyjne sieci neuronowe mają duże możliwości, to obecne aplikacje w polu wykrywania i rozpoznawania znaków drogowych nie są liczne. Przykładem jest propozycja Canyong W. w artykule \cite{T8}. Głównym powodem małej liczby rozwiązań korzystających z SI jest brak baz danych ze znakami. Trenowanie i weryfikacja głębokouczonych konwolucyjnych sieci neuronowych wymaga dużej liczby znaków w bazie danych.
Jednymi z najpopularniejszych baz danych znaków drogowych są: GTSRB oraz GRSDB w Niemczech oraz KUL w Belgii. W opisywanym artykule, autor do trenowania sieci neuronowej korzysta z sieci niemieckich. Bazy te zawierają obrazy znaków w różnych warunkach: obrócenie znaku, słabe oświetlenie, podobne kolory tła (rys. \ref{fig:sign_detection3_cnn_input}). 

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/sign_detection3_cnn_input.png}
  \caption{Różne typy znaków drogowych spotykanych na drogach\cite{T8}}
  \label{fig:sign_detection3_cnn_input}
\end{figure}

Przed przetworzeniem obrazu przez sieć neuronową jest on poddawany \textit{preprocessingowi}. Polega on na normalizacji wartości pikseli, tak by znajdowały się w przedziale $[0, 0.5]$. Robi się tak, ponieważ sieci neuronowe działają lepiej, gdy surowe wartości danych wejściowych są z zakresu $[0, 1]$. 
Zamiast konwersji do skali szarości zdecydowano, że sieć będzie działała w oparciu o pełną informację o kolorze, analogicznie do sposobu detekcji i klasyfikacji znaków przez człowieka.

Sieć neuronowa zbudowana jest (rys. \ref{fig:sign_detection3_cnn_schema})z pięciu warstw konwolucyjnych, trzech warstw fully-connected i warstwy Softmax. Po przetworzeniu obrazu przez warstwy konwolucyjne obraz jest przekazywany na wejście warstwy Softmax.

Po 20 tys. iteracji nauczania dokładność sieci wyniosła 96%.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/sign_detection3_cnn_schema.png}
  \caption{Schemat sieci neuronowej do detekcji i rozpoznawania znaków drogowych\cite{T8}}
  \label{fig:sign_detection3_cnn_schema}
\end{figure}

\subsection{Detekcja sygnalizacji świetlnej}
\label{sec:tl}
Kolejnym istotnym elementem systemów umieszczanych w pojazdach autonomicznych jest detekcja świateł drogowych.
%TODO tak się zastanawiam i nie wiem czy światła drogowe to dobra nazwa. Może sygnalizacja świetlna. Bo światła drogowe to w samochodach OK
Informują one o możliwości przejazdu przez skrzyżowanie lub rondo lub opcji znalezienia się na trasie kolizyjnej w stosunku do innych użytkowników drogi. %TODO powt. możliwości OK
Sygnalizacja świetlna spotykana jest głównie w miastach, lecz wraz z rozwojem infrastruktury widywana są także w~mniejszych miejscowościach. 
Słupy z zamontowanymi światłami mogą być widoczne na prawej lub lewej krawędzi jezdni, a także nad nią.

\subsubsection{Detekcja sygnalizacji świetlnej z użyciem informacji o kolorze i krawędziach}

Sygnalizacja świetlna na świecie jest ustandaryzowana. %TODO światła jw. OK
Istnieją trzy kolory: czerwony, żółty i zielony. 
Każdy kolor niesie za sobą informację: czerwony -- stój, żółty -- przygotuj się do zmiany z zielonego na czerwony lub odwrotnie i~zielony, który oznacza jedź. 
Jedyną znaną różnicą w stosunku do opisanego standardu jest światło pomarańczowe, występujące jako zamiennik światła żółtego w Stanach Zjednoczonych %TODO zdanie niejasne OK

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/tl_input.png}
  \caption{Obraz wejściowy algorytmu detekcji sygnalizacji świetlnej\cite{T4}}
  \label{fig:tl_input}
\end{figure}

Masako Omachi w artykule \cite{T4} proponuje algorytm, który bazuje na informacji o kolorze i~krawędziach znajdujących się na obrazie. 
Światła drogowe mają z góry ustalony kształt -- są okrągłe. 
Istnieją warianty ze strzałkami, lecz opisywany poniżej algorytm służy do detekcji świateł, które w \cite{Kodeks} mają kształt pełnego koła.

Rysunek \ref{fig:tl_input} pokazuje przykład sceny zawierającej światła drogowe. 
W~opisywanej metodzie przestrzeń barw jest konwertowana do znormalizowanej przestrzeni RGB. 
Polega ona na zmapowaniu wartości pikseli do przedziału $[0,255]$,a~także ,,rozsunięciu'' wartości pikseli na obrazie tak, by znajdowały się w całym możliwym zakresie wartości:
\begin{equation}
R=\left\{\begin{matrix}
0, &  s=0\\
\frac{r}{s} & w p.p.
\end{matrix}\right.
\end{equation}
\begin{equation}
G=\left\{\begin{matrix}
0, &  s=0\\
\frac{g}{s} & w p.p.
\end{matrix}\right.
\end{equation}
\begin{equation}
B=\left\{\begin{matrix}
0, &  s=0\\
\frac{b}{s} & w p.p.
\end{matrix}\right.
\end{equation}
gdzie:
\begin{itemize}
\item$r,g,b$ -- składowe czerwona, zielona i~niebieska nieznormalizowanego obrazu,
\item$s = r+g+b$
\item$R,G,B$ -- składowe czerwona, zielona i~niebieska znormalizowanego obrazu.
\end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/tl_norm.png}
  \caption{Efekt normalizacji przestrzeni barw\cite{T4}}
  \label{fig:tl_norm}
\end{figure}

Efekt przeniesienia przestrzeni barw do znormalizowanej przestrzeni RGB jest widoczny na rysunku \ref{fig:tl_norm}.
Następnie, poprzez progowanie każdej ze składowych barwnych uzyskuje się obszary, w których mogą znajdować się elementy sygnalizacji świetlnej. %TODO styl. OK
Decyzja o tym czy piksel należy lub nie do elementu sygnalizacji świetlnej jest podejmowana na podstawie poniższych warunków:
\begin{equation}
R>200 \wedge G< 150 \wedge B<150
\end{equation}
lub
\begin{equation}
R>200 \wedge G> 150 \wedge B<150
\end{equation} 
lub
\begin{equation}
R<150 \wedge G>240 \wedge B>220
\end{equation}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/tl_thresh.png}
  \caption{Rezultat progowania w celu wykrycia obszarów będących kandydatami do bycia sygnalizacją świetlną\cite{T4}}
  \label{fig:tl_thresh}
\end{figure}

Rezultat progowania jest widoczny na rysunku \ref{fig:tl_thresh}. 
Następnym krokiem jest wykrycie krawędzi na obrazie z kandydatami do detekcji świateł. 
Jedną z~opcji jest użycie filtru Sobela. 
Ostatnim etapem, jako że światła drogowe mają jasno określony kształt, jest użycie transformaty Hougha dla okręgów, by wykryć właściwe elementy sygnalizacji świetlnej. 
W~artykule stosowana jest zmodyfikowana transformata Hougha do wyszukiwania okręgów, która polega na ustaleniu stałej długości promienia. 
Klasyczna transformata Hougha jest opisana w podrozdziale \ref{sec:vision_algs}. %TODO sekcji ! OK

%\subsection{Wnioski i rezultaty} %TODO to nie powinno być osobne, bo dotyczy tego samego algorytmu. OK

Opisany algorytm detekcji świateł drogowych daje lepsze wyniki niż użycie standardowej przestrzeni barw i klasycznej transformaty Hougha dla okręgów. 
Porównanie jest zamieszczone w tabeli \ref{tab:tl_results}. 
Główną zaletą opisywanej metody detekcji sygnalizacji świetlnej jest fakt, że poprawnie odrzuca ona znaki drogowe, które również mają okrągły kształt i jednolite kolory na krawędziach. %TODO słowo "obrazy" nie pasuje OK
Kolejnym sprawdzanym elementem jest kolor na krawędziach, który powinien być taki sam jak wewnątrz kształtu, co pozwala skutecznie odrzucić np. znaki zakazu. %TODO "bierze" - personifikacja OK
Problemem, który napotyka algorytm są tylne światła innych pojazdów, które mają jednolity kolor (z reguły czerwony), a także kształt zbliżony do czerwonego światła sygnalizatora. %TODO kształt ? OK
Przykładowy błąd detekcji jest ukazany na rysunku \ref{fig:tl_err}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/tl_err.png}
  \caption{Przykład błędnej detekcji tylnego światła traktora\cite{T4}}
  \label{fig:tl_err}
\end{figure}

\begin{table}[]
\centering
\caption{Porównanie algorytmu detekcji świateł opartego na transformacie Hougha i opisanego w pracy\cite{T4}} %TODO W pracy X OK
\begin{tabular}{lllll}
\cline{1-3}
\multicolumn{1}{|l|}{}                           & \multicolumn{1}{l|}{Algorytm klasyczny} & \multicolumn{1}{l|}{Algorytm opisany w tym rozdziale} &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Dokładność}                 & \multicolumn{1}{l|}{20/30}              & \multicolumn{1}{l|}{26/30}                            &  &  \\ \cline{1-3}
\multicolumn{1}{|l|}{Czas przetwarzania {[}s{]}} & \multicolumn{1}{l|}{0.561}              & \multicolumn{1}{l|}{0.347}                            &  &  \\ \cline{1-3}
                                                 &                                         &                                                       &  & 
\end{tabular}
\label{tab:tl_results}
\end{table}
%TODO a to co jest alg. klasyczy. OK

%TODO konkurencyjna metoda OK
%TODO NN - są fajnie prace, łatwo Pan znajdzie. OK

\subsubsection{Alternatywne rozwiązanie}
Inną propozycję zaprezentowano w pracy \cite{T9}. Detekcja sygnalizacji świetlnej odbywa się z użyciem cech Haara.
Większość aplikacji jest oparta o informacje o kolorze, jednakże sprawia to, że systemy mogą nie być odporne na zmienne czynniki takie jak kąt widzenia, intensywność świecenia lub poziom jasności tła.
Rysunek \ref{fig:traffic_light2_input} pokazuje, że obraz wejściowy mimo przekazywania takiej samej informacji może się znacząco różnić wartościami koloru.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light2_input.png}
  \caption{Przykład wariancji danych wejściowych\cite{T9}}
  \label{fig:traffic_light2_input}
\end{figure}

Cechy Haara są używane aby wykryć sygnalizację świetlną. Metoda korzysta z widocznych różnic w jasności pomiędzy poszczególnymi sektorami obrazka.
Dla określonego regionu wyznaczane są cechy Haara na podstawie masek przedstawionych na rysunku \ref{fig:maski_maseczki}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/maski_maseczki.png}
  \caption{Maski używane do wyznaczania cech Haara\cite{T9}}
  \label{fig:maski_maseczki}
\end{figure}

Cechy Haara zawierają informację o kształcie obiektu i używają różnicy jasności, a nie jasności samej w sobie, więc są odporne na niewielką zmianę kształtu  i pozycji na obrazie.

Różnica jasności w sygnalizatorze jest z reguły taka sama. Istotne jest, aby do ROI zawierającego światła dodać tło. Jest to ważne z punktu widzenia klasyfikatora SVM, któremu dostarcza się zarówno próbek pozytywnych (obrazy ze światłami) oraz negatywnych (samo tło).

Klasyfikator oparty na maszynie wektorów wspierających jest uczony w oparciu o obrazy uzyskane z detekcji za pomocą cech Haara. Zbiór treningowy zawiera zarówno próbki pozytywne i negatywne. SVM uczy się dwóch klas (rys. \ref{fig:traffic_light2_svm}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light2_svm.png}
  \caption{Klasyfikator SVM\cite{T9}}
  \label{fig:traffic_light2_svm}
\end{figure}

Na wycinku obrazu, który zawiera sygnalizację świetlną przeprowadza się operację binaryzacji. Następnie wykonywane są inne operacje morfologiczne, których celem jest redukcja szumu oraz mocniejsze zaznaczenie sygnalizatora. Seria wykonywanych operacji jest widoczna na rysunku \ref{fig:traffic_light2_morph}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light2_morph.png}
  \caption{Rezultat operacji morfologicznych\cite{T9}}
  \label{fig:traffic_light2_morph}
\end{figure}

Operacja wykrywania konturów jest wykonywana na obrazie po operacjach morfologicznych. Rezultat detekcji jest umieszczony na rysunku \ref{fig:traffic_light2_result}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light2_results.png}
  \caption{Wykryte sygnalizatory świetlne\cite{T9}}
  \label{fig:traffic_light2_result}
\end{figure}

\subsubsection{Rozwiązanie z użyciem deep learnigu}

Kombinacja sygnalizatorów świetlnych i znaków drogowych na jezdni pomaga zapobiegać chaosowi. W przypadku samochodów autonomicznych detekcja tych elementów w połączeniu z dobrym sterowaniem nimi pozwoli na znaczne upłynnienie ruchu.
Podobnie jak w przypadku znaków drogowych i linii pojawia się problem zmienności warunków otoczenia. 
W przypadku tradycyjnego podejścia niewielka zmiana parametrów środowiska może powodować błędne detekcje, co jest wysoce niepożądane w przypadku opisywanego systemu.
W pracy \cite{T10} zaproponowano rozwiązanie opierające się na konwolucyjnej sieci neuronowej.
Treningowy zbiór danych jest utworzony na podstawie danych zebranych na indyjskich drogach. Preprocessing polega na przeskalowaniu obrazów do rozmiaru 800x600 pikseli. Wybrano pięć klas, do których mogą zostać zakwalifikowane detekcje: światło czerwone, żółte, zielone, jazda w lewo, jazda w prawo. 
Na każdym ze zdjęć ręcznie zaznaczono typ sygnalizatora, który zawiera (rys. \ref{fig:traffic_light3_labels}).

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light3_labels.png}
  \caption{Oznaczone sygnalizatory świetlne (po lewej) i zdjęcia wejściowe w pełnej skali (po prawej)\cite{T10}}
  \label{fig:traffic_light3_labels}
\end{figure}

Sieć neuronową zbudowano w oparciu o \textit{transfer learning}, czyli uczenie sieci wstępnie nauczonej. Pozwoliło to na zredukowanie potrzebnego czasu i zasobów na nauczenie sieci od początku. 
Po 120 tys. iteracji procesu uczenia sieci zakończono uczenie sieci. Przykładowy rezultat detekcji i rozpoznawania sygnalizacji drogowej jest umieszczony na rysunku \ref{fig:traffic_light3_results}.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/traffic_light3_results.png}
  \caption{Przykładowa detekcja świateł zielonych\cite{T10}}
  \label{fig:traffic_light3_results}
\end{figure}





\subsection{Detekcja samochodu poprzedzającego}
\label{sec:car_general}

Istotnym zadaniem stawianym przed algorytmami wizyjnymi stosowanymi w~pojazdach autonomicznych jest wykrycie samochodów w najbliższym otoczeniu pojazdu. 
Detekcja może być wspierana odczytami z radaru lub lidaru, jednak w~poniższym podrozdziale zostanie opisany algorytm bazujący jedynie na obrazie z kamery. %TODO 1. powt. detekcja, sekcja !, kamery, czy kamer OK, kamery, są algorytmy, które składają obraz z dwóch kamer, ale faktycznie opisywany alg. korzysta z jednej
Większość współczesnych samochodów widzianych od tyłu zachowuje symetrię względem pionowej osi przechodzącej przez środek pojazdu. 

Pierwszym, wstępnym krokiem jest zbadanie możliwych pozycji samochodów na obrazie i oznaczenie ich jako ROI.
Dla systemu z~fuzją danych wizyjnych i radarowych może to być zrobione poprzez analizę odległości i prędkości względnej, czyli danych uzyskanych z radaru. 
Dla systemu, który posiada jedną kamerę, pozycja samochodów musi być wyznaczona tylko na podstawie ruchu samochodów na obrazie w czasie.

Opisywany algorytm korzysta z detektora symetrii, który działa w następujący sposób. 
Dla każdego piksela wyznaczana jest liczba punktów, która jest wartością bezwzględną z różnicy wartości pikseli, które są równoodległe od ustalonej osi symetrii(równanie \ref{eq:sym_detect}). %TODO niejasne - rysunek - dałem odnośnik do równania w dalszej części tekstu, które lepiej obrazuje sposób działania detektora
Jest to zwykle robione z użyciem pewnego okna o z góry ustalonym rozmiarze dobieranym tak, aby pasować do rozmiaru samochodów, które mogą znajdować się na obrazie. 
Będąc świadomym faktu, że samochód im jest dalej od kamery, tym jest mniejszy zastosowano kilka predefiniowanych rozmiarów okien. 
Wartości wskaźnika symetrii mogą być obliczane dla każdego punktu na obrazie. 
Piksele z dużą wartością tego wskaźnika są kandydatami do należenia do osi symetrii. %TODO styl. OK
Wyliczanie wskaźnika symetrii dla każdego punktu na obrazie jest bardzo czasochłonne. Zdecydowano się na jego wyznaczanie tylko na wcześniej określonych poziomych liniach, które pokrywają obszar, na którym mogą znajdować się samochody. %TODO styl. na dwa zdania i zgrabniej OK
Do obliczania wskaźnika symetrii może być użytych kilka cech obrazu takich jak: wartości pikseli w skali szarości, obraz krawędzi, składowa S w przestrzeni barw HSV. 
Szukanie obrazu samochodu na obrazie w skali szarości jest szybsze, jednak wrażliwe na zmiany oświetlenia (noc, deszcz). 
Dobrze sprawdza się badanie nasycenia w przestrzeni barw HSV, ponieważ uniezależnia to obraz samochodu od ogólnej jasności otoczenia i~częściowo od pogody.

\subsubsection{Przebieg algorytmu bazującego na operatorze symetrii}
%TODO To całe to powinnien być podrodział, a to ew. subsub lub paragraph. OK
Pierwszym krokiem jest wygenerowanie obrazu krawędzi na podstawie obrazu w skali szarości lub składowej S przestrzeni barw HSV. 
W~opisanym algorytmie zaproponowano detektor Canny'ego. 
Rysunek \ref{fig:car_edge} pokazuje rezultat wykrywania krawędzi dla typowego obrazu zawierającego samochód poprzedzający. 
Opierając się na znanej pozycji kamery i jej pochyleniu względem nawierzchni drogi można określić obszar na obrazie, na którym będą szukane samochody. 
Poziome ograniczenia znajdują się pomiędzy horyzontem i początkiem widocznej drogi na dole obrazu. 
Pionowe ograniczenia są ustawione tak, by odpowiadać lewemu i prawemu ograniczeniu jezdni. 

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_canny.png}
  \caption{Obraz z samochodami po filtracji filtrem Canny'ego\cite{T1}}
  \label{fig:car_edge}
\end{figure}

Jak wspomniano w sekcji \ref{sec:car_general}, w celu zredukowania czasu obliczeń nie każdy piksel w wybranym obszarze jest analizowany. 
Obliczenia dotyczące symetrii są przeprowadzane tylko dla 15 równoodległych linii skanu (rys. \ref{fig:car_scan_lines1}). 
Obraz jest przeskalowywany do rozdzielczości 600x480, więc wejściowa rozdzielczość obrazu nie ma znaczenia dla obliczeń. %TODO no pozioma jednak ma.... - chodzi o to, że obraz może być przeskalowany do pewnej założonej rozdzielczości.
Dzieje się tak ponieważ algorytm wykrywa tylko maksima wzdłuż linii skanu. Autorzy algorytmu zalecają przeskalowanie obrazu do niższej rozdzielczości w celu zredukowania czasu przetwarzania.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_lines.png}
  \caption{Obraz z samochodami po filtracji filtrem Canny'ego i zaznaczonymi liniami skanu\cite{T1}}
  \label{fig:car_scan_lines1}
\end{figure}

Kolejnym krokiem jest detekcja symetrii. 
Jest ona przeprowadzana dla każdego punktu leżącego na linii skanu.
Wartość operatora symetrii dla piksela wyraża się wzorem:

\begin{equation}
\label{eq:sym_detect}
SymVal(x,y)=\sum_{x'=1}^{W/2}\sum_{y'=y-H/2}^{y+H/2}S(x, x', y')
\end{equation}
gdzie:
\begin{itemize}
\item
\begin{equation}
S(x,x',y')=\begin{cases}
2 & \text{ gdy } I(x-x',y')=I(x+x',y')=1 \\ 
-1 & \text{ gdy } I(x-x',y')\neq I(x+x',y') \\ 
0 & \text{ w p.p. }
\end{cases}
\end{equation}
\item $W$ -- szerokość okna
\item $H$ -- wysokość okna
\item $I(x,y)$ -- wartość piksela o współrzędnych $x,y$
\end{itemize}

Szerokość okna powinna być właściwie ustawiona, aby poprawnie wykrywać symetryczne obiekty o różnych rozmiarach. 
W~trakcie eksperymentów wykazano, że wartości dające poprawne rezultaty $W$ mieszczą się w przedziale $[8,12]$. %TODO optymalne...to nie dobre słowo po AiR w tym kontekście. OK - ciężko znaleźć właściwy wskaźnik jakości

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/tl_peaks.png}
  \caption{Wartości wskaźnika symetrii wyznaczone dla linii skanu\cite{T1}}
  \label{fig:car_scan_lines}
\end{figure}

Na rysunku \ref{fig:car_scan_lines} widać, że w niektórych punktach istnieją maksima, które wskazują, że dany punkt może należeć do osi symetrii. 
W kolejnym kroku wybiera się maksima i stosuje progowanie, to znaczy wartości maksimów lokalnych poniżej pewnej wartości są odrzucane. %TODO W kolejnym kroku... OK
Zwykle wartości poniżej określonego progu wskazują na małe, symetryczne elementy tła. 
Progowanie dokonuje się według następującej formuły:
\begin{equation}
SymPts(x,y)=\begin{cases}
1 & \text{ gdy } SymVal(x,y)>T\\ 
0 & \text{ w p.p.}
\end{cases}
\end{equation}

gdzie:
\begin{itemize}
\item $T$ - ustalony próg odrzucenia maksimum lokalnego
\end{itemize}


\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_symmetry.png}
  \caption{Wykryte osie symetrii na liniach skanu\cite{T1}}
  \label{fig:car_detected}
\end{figure}

Ostatecznie znalezione maksima oznaczają wykryte osie symetrii względnie dużych obiektów, w tym przypadku samochodów. 
Widać to na rysunku \ref{fig:car_detected}. 
Dla każdej linii skanu wykryta oś symetrii jest przesunięta o kilka pikseli, dlatego ostatnim etapem detekcji samochodu jest klasteryzacja.

Uzyskane punkty osi symetrii są klasteryzowane metodą k-średnich. 
Liczba samochodów na obrazie jest nieznana, więc klasteryzację przeprowadza się iteracyjnie, co iterację licząc wariancję, która przy poprawnej liczbie klastrów w stosunku do samochodów na obrazie będzie mniejsza niż określony próg. 
Końcowy wynik z zaznaczonymi środkami samochodów jest widoczny na rysunku \ref{fig:car_end}

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_end.png}
  \caption{Wynik algorytmu detekcji samochodów poprzedających\cite{T1}}
  \label{fig:car_end}
\end{figure}

\subsubsection{Alternatywne rozwiązanie}
Na drodze oprócz detekcji samochodu poprzedzającego przydatny jest również sposób na detekcję samochodu, który znalazł się w martwym polu. W samochodach poziomu zerowego pojawiają się asystenci martwego pola, który informują, że może znajdować się tam samochód.
Algorytm detekcji zaproponowany w \cite{T11} pozwala wykryć samochody znajdujące się w martwej strefie (rys. \ref{fig:car_detection2_blind_spot2})

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_detection2_blind_spot.png}
  \caption{Położenie martwego pola\cite{T11}}
  \label{fig:car_detection2_blind_spot2}
\end{figure}

Obraz pozyskiwany jest z kamer umieszczonych w lusterkach. Następnie jest skalowany do jednej czwartej pierwotnego rozmiaru, aby zredukować czas przetwarzania. 
Następnym krokiem jest detekcja samochodu, która wykonywana jest periodycznie. Pomiędzy detekcjami auta, wykonywane jest śledzenie z użyciem filtru Kalmana. 
Wyznaczane jest ROI, gdzie można spodziewać się nadjeżdżającego samochodu. Wycinany jest fragment nieba oraz asfaltu blisko pojazdu.
Na wyznaczonym obszarze wykonywana jest detekcja pojazdu z użyciem filtru kaskadowego, który jest szybszy niż klasyfikator SVM.
Wytrenowano klasyfikator HoG (ang. Histogram of Gradients - histogram gradientów), który daje mniej fałszywych detekcji niż LPB (ang. Local Binary Pattern - lokalny wzór binarny).
Zbiór treningowy zawierał 7500 próbek pozytywnych oraz 20000 próbek negatywnych. Okno treningowe ma rozmiar 32x32 pikseli. Wykryte pojazdy są śledzone za pomocą filtru Kalmana. Filtr inicjalizuje swój stan na podstawie detekcji samochodu i przewiduje jego pozycję w następnej klatce.
Aby zwiększyć skuteczność algorytmu stosuje się następujące rozumowanie: jeżeli samochód był wykrywany w poprzednich klatkach, algorytm zakłada, że samochód istnieje, chociaż w aktualnej klatce w przewidywanym obszarze nie został wykryty. Działanie odwrotne jest również stosowane: jeżeli samochód w kilku poprzednich klatkach nie był wykryty, a pojawia się np. w środku ROI, to prawdopodobnie jest to fałszywa detekcja. 

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/car_detection2_result.png}
  \caption{Wynik działania algorytmu wykrywającego pojazd w martwej strefie\cite{T11}}
  \label{fig:car_detection2_result}
\end{figure}

Średni czas przetwarzania jednej klatki (uwzględniając wykrywanie pojazdu na co piątym obrazie) wyniósł 24.5 ms. Przykładowy rezultate detekcji znajduje się na rysunku (\ref{fig:car_detection2_result}).
W porównaniu do algorytmu opisanego we wcześniejszym podrozdziale, którego mocną wadą była niska wydajność, dobrym pomysłem wydaje się wykrywanie pojazdów i śledzenie ich na kilku kolejnych klatkach z użyciem np. filtru Kalmana.

%TODO drugie podejście - ciężko o algorytm tradycyjny - opisałem detekcję blind spota, który również zawiera elementy ML
%TODO podejście NN jw.

%TODO Proszę już nie dopisywać osobnego rodziału, ale gdzieś wspomnieć np. o detekcji pieszych, czy rowerzystów... - z uwagi na "biedniejszy" rozdział o samochodach dodam podrozdział o detekcji pieszych i rowerzystów


\subsection{Wykrywanie innych obiektów na drodze}
Oprócz samochodów na drogach zauważalne są inne obiekty takie jak piesi, rowerzyści lub motocykliści. Aby zapobiegać licznym wypadkom z ich udziałem globalne firmy jak np. Volvo zaproponowały zaawansowane systemy wspomagania kierowcy (ADAS).
Kamery, radary lub LIDARy są używane w celu zwizualizowania otoczenia pojazdu. Wykrywanie obiektów w czasie rzeczywistym stało się istotną rzeczą, która realnie wpływa na bezpieczeństwo kierowcy i innych użytkowników ruchu.
Artykuł \cite{T12} sugeruje użycia \textit{SSD} (ang. Single Shot Detector), który pokonuje problemy wspomniane wcześniej, a więc umożliwia detekcję obiektów niewrażliwą na zmienne warunki środowiska.
Działanie SSD jest oparte na głębokouczonych sieciach neuronowych. Z obrazów wejściowych generuje się macierze cech przy użyciu warstw konwolucyjnych. 
Jak pokazano na rys. \ref{fig:pedestrian_detect_result} detekor jest w stanie wykryć różne klasy obiektów.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/pedestrian_detect_result.png}
  \caption{Wynik działania systemu opartego o SSD. Wydoczne wykryte różne klasy obiektów \cite{T11}}
  \label{fig:pedestrian_detect_result}
\end{figure}

Słabą stroną SSD jest fakt, że korzysta on z siatki, pomijając niektóre obszary, co sprzyja pomijaniu niewielkich obiektów.

Opisany system znajduje zastosowanie w systemach ADAS (na przykładzie Audi A8 z 2019 roku):
\begin{itemize}
\item Nightvision -- detekcja pieszych i rowerzystów przy użyciu światła podczerwonego
\item Pedestrian Assist -- badanie pozycji pieszych wokół samochodu i ewentualne ostrzeganie o niebezpiecznej sytuacji
\item Adaptive Cruise Control -- w połączeniu z danymi radarowymi obsługa aktywnego tempomatu
\end{itemize}


\section{Opis wybranych zagadnień i algorytmów przetwarzania obrazu}
\label{sec:vision_algs}
W tym podrozdziale zostaną opisane podstawowe algorytmy i zagadanienia dotyczące cyfrowego przetwarzania obrazów, które są używane w zaawansowanych algorytmach wizyjnych w pojazdach autonomicznych. %TODO "sekcja". OK

\subsection{Transformata Hougha dla okręgów}

Systemy wizyjne w pojazdach autonomicznych często mają za zadanie wykrycie obiektów o kształcie koła. Algorytmem do tego przeznaczonym jest transformata Hougha. 
Istnieje ona w wersji do detekcji prostych i okręgów. 
Pod pojęciem uogólniona transformata Hougha należy rozumieć algorytm służący do detekcji dowolnego zadanego konturu.
%TODO "kryje się" zły styl OK

Okrąg można sparametryzować za pomocą następującego wzoru:
\begin{equation}
(x-x_0)^2+(y-y_0)^2=r^2
\end{equation}
gdzie:
\begin{itemize}
\item $x_0, y_0$ -- współrzędne środka okręgu,
\item $r$ -- promień okręgu.
\end{itemize}

\begin{figure}[h]
\centering
\includegraphics[width=7cm]{img/hough.png}
\caption{Linia w układzie współrzędnych określona za pomocą parametrów $(r, \theta)$}
\label{fig:hough}
\end{figure}
%TODO brak odniesienia w txt ok

Jeżeli promień będzie ustalony, to okrąg zostanie sparametryzowany za pomocą dwóch liczb. 
Gdy szukamy okręgów o nieznanych promieniach, rośnie złożoność obliczeniowa, ponieważ wymiar przestrzeni parametrów zwiększa się o jeden.

Możliwa jest również parametryzacja okręgu w biegunowym układzie współrzędnych (rys. \ref{fig:hough}):
\begin{equation}
x = x_0 + rcos(\theta)
\end{equation}
\begin{equation}
y = y_0 + rsin(\theta)
\end{equation}

po prostym przekształceniu otrzymuje się:

\begin{equation}
x_0 = x - rcos(\theta)
\end{equation}
\begin{equation}
y_0 = y - rsin(\theta)
\end{equation}

Następnie wyznaczana jest przestrzeń Hougha. W niej akumulowane są wartości oznaczające liczbę okręgów, które przechodzą przez dany piksel na obrazie wejściowym. Ten proces nazywa się głosowaniem. 
Wartości maksymalne w przestrzeni Hougha oznaczają wykryty okrąg.

%TODO brakuje wyjaśnienia jak działa ta przestrzeń Hougha OK

\subsection{Przestrzenie barw}

Podstawową przestrzenią barw jest RGB. 
Przestawiona jest na rysunku \ref{fig:rgb} za pomocą sześcianu. 
Każda składowa jest odpowiedzialna za informację o zawartości danego koloru. 
Jej zaletą jest prostota opisu, natomiast wadą jest fakt, że po niewielkiej zmianie wartości składowych otrzymuje się zupełnie inną barwę. 
Dodatkowo, co jest ważne w przypadku systemów wizyjnych, niewielka zmiana poziomu jasności powoduje duże wahania składowych R, G, B.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/rgb.jpg}
  \caption{Sześcian przedstawiający przestrzeń barw RGB\cite{W4}}
  \label{fig:rgb}
\end{figure}


\begin{figure}
  \centering
  \includegraphics[width=7cm]{img/hsv.jpg}
  \caption{Stożek przedstawiający przestrzeń barw HSV(\textit{źródło: Wikipedia})}
  \label{fig:hsv}
\end{figure}

Drugą, ważną przestrzenią barw używaną w cyfrowym przetwarzaniu obrazów jest przestrzeń HSV. 
Jest ona przedstawiona na rysunku \ref{fig:hsv} za pomocą stożka. 
Jej główną zaletą jest to, że przy niewielkich zmianach jasności zauważalne są niewielkie zmiany składowej S. %TODO styl. OK
Pozwala to na uniezależnienie się w pewnym stopniu od czynników takich jak pora dnia lub pogoda.

\subsection{Filtr Canny'ego}
Podstawowym, dobrze sprawdzającym się detektorem krawędzi, jest filtr Canny'ego \cite{T5}. 
Cechuje się następującymi właściwościami: %TODO by se przydało źródło (choćby oryginalny artykuł Cannego) OK

\begin{itemize}
\item niska liczba fałszywych detekcji krawędzi,
\item poprawne wskazywanie pozycji krawędzi. Pozycja krawędzi wskazywana przez detektor powinna odpowiadać jej prawdziwemu położeniu
\item jedna wykryta krawędź przypadająca na rzeczywistą krawędź
\end{itemize}

Detektor krawędzi Canny'ego jest algorytmem wieloetapowym:
\begin{enumerate}
\item Redukcja szumów z obrazu. Używany jest filtr Gaussa. Przykładowa macierz filtru pokazana jest na rysunku \ref{fig:canny_gauss}. %TODO to jakichkolwiek to przesada - redukcja szumu OK
\begin{figure}[h]
\centering
\includegraphics[scale=0.8]{img/canny_gauss.png}
\caption{Maska filtru Gaussa stosowana w wykrywaniu krawędzi}
\label{fig:canny_gauss}
\end{figure}
\item Wyszukiwanie krawędzi z użyciem filtru Sobela o poziomej i pionowej orientacji.
\item Określenie wartości gradientu i jego kierunku:

\begin{equation}
G=\sqrt{G_x^2+G_y^2}
\end{equation}

\begin{equation}
\theta=arctan(\frac{G_y}{G_x})
\end{equation}
Kierunek jest kwantyzowany na jeden z czterech możliwych kierunków: \ang{0}, \ang{45}, \ang{90}, \ang{135}.
%TODO może lepiej kwantyzowany ? OK

\item Wartości gradientu w kierunku prostopadłym do głównego przebiegu wykrytej krawędzi o niemaksymalnych wartościach są usuwane. %TODO dodać w jakim otoczeniu OK
Ma to na celu usunięcie pikseli, które prawdopodobnie nie są elementem krawędzi. 
W wyniku tej operacji pozostają tylko cienkie linie jako krawędzie.

\item Filtr Canny'ego jako argumenty otrzymuje dwa progi -- górny i dolny:
\begin{itemize}
\item jeżeli wartość gradientu przekracza górny próg, piksel jest zawsze uznawany jako krawędź,
\item jeżeli wartość gradientu nie przekracza dolnego progu, piksel nie jest uznawany za krawędź,
\item jeżeli wartość gradientu jest pomiędzy dwoma progami, jest krawędzią, tylko wtedy gdy jest połączony z pikselem, który został sklasyfikowany jako krawędź.
\end{itemize}
Twórca filtru rekomenduje stosunek progów filtru pomiędzy 2:1 i 3:1
\end{enumerate}